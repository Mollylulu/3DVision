## SOTA Techniques of Perception with Multiples Cameras

This repository provides an up-to-date list of techniques used for autolabeling project. 
 
=============================
# Table of contents
1.  [DataSets and Benchmarks](#1)  
    1.1 [3D ObjectS Detection](#1.1)  
    1.2 [Lane Segmentation](#1.2)    
2. [3D Obstacles Detection](#2)  
    2.1 [Bird's-Eye-View Based](#2.1)  
    2.2 [Multi-Modals Fusion](#2.2)    
3. [Road Markers Detection](#3)  
    3.1 [Lanes Segmentation](#3.1)  
    3.2 [Lights and other Signs Detection](#3.2)  

----------------------------------
# 1. DataSets and Benchmarks <a name="1"></a>  
## 1.1 3D ObjectS Detection<a name="1.1"></a>
  - nuscenes a multimodal dataset for autonomous driving; [[Paper]](https://arxiv.org/pdf/1903.11027.pdf) [[Project]](https://www.nuscenes.org/) 
  - Argoverse: 3D Tracking and Forecasting with Rich Maps; [[Paper]](https://arxiv.org/pdf/1911.02620.pdf) [[Project]](https://www.argoverse.org/) 
  - Data - Lyft's Level 5; [[Paper]](https://arxiv.org/pdf/2006.14480.pdf) [[Project]](https://level-5.global/data/)
  - Scalability in Perception for Autonomous Driving: Waymo Open Dataset; [[Paper]](https://arxiv.org/pdf/1912.04838.pdf) [[Project]](https://waymo.com/open/) 
  
## 1.2 Lane Segmentation<a name="1.2"></a>  
 - PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark; [[Paper]](https://arxiv.org/pdf/2203.11089.pdf) [[Project]](https://github.com/OpenPerceptionX/PersFormer_3DLane) [[Datasets]](https://github.com/OpenPerceptionX/OpenLane)  
 - Spatial As Deep: Spatial CNN for Traffic Scene Understanding; [[Paper]](https://arxiv.org/pdf/1712.06080.pdf) [[Project]](https://xingangpan.github.io/projects/CULane.html)  
 - tusimple benchmark; [[Project]](https://github.com/TuSimple/tusimple-benchmark/tree/master/doc/lane_detection)   
 - ONCE-3DLanes: Building Monocular 3D Lane Detection; [[Paper]](https://arxiv.org/pdf/2205.00301.pdf) [[Project]](https://once-3dlanes.github.io/)  
 - GitNet: Geometric Prior-based Transformation for Birds-Eye-View Segmentation ; [[Paper]](https://arxiv.org/pdf/2204.07733.pdf) 

# 2 3D Obstacles Detection <a name="2"></a>   
## 2.1 Bird's-Eye-View Based<a name="2.1"></a>  
  - M2BEV: Multi-Camera Joint 3D Detection and Segmentation with Unified Bird’s-Eye View Representation; [[Paper]](https://arxiv.org/pdf/2204.05088.pdf) [[Project]](https://xieenze.github.io/projects/m2bev/) 
  - HFT: Lifting Perspective Representations via Hybrid Feature Transformation; [[Paper]](https://arxiv.org/pdf/2204.05068.pdf) [[Project]](https://github.com/JiayuZou2020/HFT) 
  - BEVFormer: Learning Bird’s-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers; [[Paper]](https://arxiv.org/pdf/2203.17270.pdf)[[Project]](https://github.com/zhiqi-li/BEVFormer)
  - PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark; [[Paper]](https://arxiv.org/pdf/2203.11089.pdf) [[Project]](https://github.com/OpenPerceptionX/PersFormer_3DLane) [[Datasets]](https://github.com/OpenPerceptionX/OpenLane)
   - BEVSegFormer: Bird’s Eye View Semantic Segmentation From Arbitrary Camera Rigs; [[Paper]](https://arxiv.org/pdf/2203.04050.pdf)
   - BEVDet: High-Performance Multi-Camera 3D Object Detection in Bird-Eye-View; [[Paper]](https://arxiv.org/pdf/2112.11790.pdf)
## 2.2 Multi-Modals Fusion<a name="2.2"></a>   
   - MVFuseNet: Improving End-to-End Object Detection and Motion Forecasting through Multi-View Fusion of LiDAR Data; [[Paper]](https://arxiv.org/pdf/2104.10772.pdf)
 
# 3 Road Markers Detection <a name="3"></a>   
## 3.1 Lanes Segmentation<a name="3.1"></a>   
## 3.2 Lights and other Signs Detection<a name="3.2"></a>   


 
